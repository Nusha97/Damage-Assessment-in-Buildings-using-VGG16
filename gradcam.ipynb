{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "96oqB_zGluee",
    "outputId": "1457df6e-6d4f-45f1-fed9-cada58d09936"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-51a1cc6c4a63>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git clone https://github.com/Ankush96/grad-cam.tensorflow/\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git clone https://github.com/Ankush96/grad-cam.tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "AkWXiCuNpSIo",
    "outputId": "9266c1f7-effb-43bb-fd2b-70f005bdc8f7"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-24553932d9e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimagenet_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'imread'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from vgg import vgg16\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from matplotlib import pyplot as plt\n",
    "from imagenet_classes import class_names\n",
    "from scipy.misc import imread, imresize\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string(\"input\", \"laska.png\", \"Path to input image ['laska.png']\")\n",
    "flags.DEFINE_string(\"output\", \"laska_save.png\", \"Path to input image ['laska_save.png']\")\n",
    "flags.DEFINE_string(\"layer_name\", \"pool5\", \"Layer till which to backpropagate ['pool5']\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def load_image(img_path):\n",
    "print(\"Loading image\")\n",
    "img = imread(img_path, mode='RGB')\n",
    "img = imresize(img, (224, 224))\n",
    "# Converting shape from [224,224,3] tp [1,224,224,3]\n",
    "x = np.expand_dims(img, axis=0)\n",
    "# Converting RGB to BGR for VGG\n",
    "x = x[:,:,:,::-1]\n",
    "return x, img\n",
    "\n",
    "\n",
    "def grad_cam(x, vgg, sess, predicted_class, layer_name, nb_classes):\n",
    "print(\"Setting gradients to 1 for target class and rest to 0\")\n",
    "# Conv layer tensor [?,7,7,512]\n",
    "conv_layer = vgg.layers[layer_name]\n",
    "# [1000]-D tensor with target class index set to 1 and rest as 0\n",
    "one_hot = tf.sparse_to_dense(predicted_class, [nb_classes], 1.0)\n",
    "signal = tf.mul(vgg.layers['fc3'], one_hot)\n",
    "loss = tf.reduce_mean(signal)\n",
    "\n",
    "grads = tf.gradients(loss, conv_layer)[0]\n",
    "# Normalizing the gradients\n",
    "norm_grads = tf.div(grads, tf.sqrt(tf.reduce_mean(tf.square(grads))) + tf.constant(1e-5))\n",
    "\n",
    "output, grads_val = sess.run([conv_layer, norm_grads], feed_dict={vgg.imgs: x})\n",
    "output = output[0]           # [7,7,512]\n",
    "grads_val = grads_val[0]\t # [7,7,512]\n",
    "\n",
    "weights = np.mean(grads_val, axis = (0, 1)) \t\t\t# [512]\n",
    "cam = np.ones(output.shape[0 : 2], dtype = np.float32)\t# [7,7]\n",
    "\n",
    "# Taking a weighted average\n",
    "for i, w in enumerate(weights):\n",
    "    cam += w * output[:, :, i]\n",
    "\n",
    "# Passing through ReLU\n",
    "cam = np.maximum(cam, 0)\n",
    "cam = cam / np.max(cam)\n",
    "cam = resize(cam, (224,224))\n",
    "\n",
    "# Converting grayscale to 3-D\n",
    "cam3 = np.expand_dims(cam, axis=2)\n",
    "cam3 = np.tile(cam3,[1,1,3])\n",
    "\n",
    "return cam3\n",
    "\n",
    "\n",
    "def main(_):\n",
    "x, img = load_image(FLAGS.input)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(\"\\nLoading Vgg\")\n",
    "imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "vgg = vgg16(imgs, 'vgg16_weights.npz', sess)\n",
    "\n",
    "print(\"\\nFeedforwarding\")\n",
    "prob = sess.run(vgg.probs, feed_dict={vgg.imgs: x})[0]\n",
    "preds = (np.argsort(prob)[::-1])[0:5]\n",
    "print('\\nTop 5 classes are')\n",
    "for p in preds:\n",
    "    print(class_names[p], prob[p])\n",
    "\n",
    "# Target class\n",
    "predicted_class = preds[0]\n",
    "# Target layer for visualization\n",
    "layer_name = FLAGS.layer_name\n",
    "# Number of output classes of model being used\n",
    "nb_classes = 1000\n",
    "\n",
    "cam3 = grad_cam(x, vgg, sess, predicted_class, layer_name, nb_classes)\n",
    "\n",
    "img = img.astype(float)\n",
    "img /= img.max()\n",
    "\n",
    "# Superimposing the visualization with the image.\n",
    "new_img = img+3*cam3\n",
    "new_img /= new_img.max()\n",
    "\n",
    "# Display and save\n",
    "io.imshow(new_img)\n",
    "plt.show()\n",
    "io.imsave(FLAGS.output, new_img)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "id": "s4O8FWmDl3Jx",
    "outputId": "f2c7d0c9-eaa4-438b-d784-d02a060205bb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/content/gdrive/My Drive/MS Applications/Georgia Tech/DIP/Project/grad-cam.tensorflow-master/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimagenet_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'imread'"
     ]
    }
   ],
   "source": [
    "%run main.py --input laska.png --output laska_save.png --layer_name pool5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "b-3r3JRxZXJz",
    "outputId": "8183ede5-1ded-44b8-b1d8-1d5d613c6b7a"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-1279b20700ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;31m#predicted_class = history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m \u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"block5_conv3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradcam.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-1279b20700ef>\u001b[0m in \u001b[0;36mgrad_cam\u001b[0;34m(input_model, image, category_index, layer_name)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mnb_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    155\u001b[0m       raise TypeError('The added layer must be '\n\u001b[1;32m    156\u001b[0m                       \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                       'Found: ' + str(layer))\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: <keras.engine.training.Model object at 0x7f888e39d588>"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import (\n",
    "    VGG16, preprocess_input, decode_predictions)\n",
    "from keras.preprocessing import image\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.framework import ops\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "def target_category_loss(x, category_index, nb_classes):\n",
    "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
    "\n",
    "def target_category_loss_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "def load_image(path):\n",
    "    #img_path = \n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='block5_conv3'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "def modify_backprop(model, name):\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = VGG16(weights='imagenet')\n",
    "    return new_model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def grad_cam(input_model, image, category_index, layer_name):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(input_model)\n",
    "\n",
    "    nb_classes = 1000\n",
    "    #x = tf.loadLayersModel(model)\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    #target_layer = target_category_loss(layer_name, category_index, nb_classes)\n",
    "    model.add(Lambda(target_layer,\n",
    "                     output_shape = target_category_loss_output_shape))\n",
    "    #print(output_shape)\n",
    "\n",
    "    loss = K.sum(model.layers[-1].output)\n",
    "    conv_output =  [l for l in model.layers[0].layers if l.name is layer_name][0].output\n",
    "    grads = normalize(K.gradients(loss, conv_output)[0])\n",
    "    gradient_function = K.function([model.layers[0].input], [conv_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "\n",
    "    weights = np.mean(grads_val, axis = (0, 1))\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * output[:, :, i]\n",
    "\n",
    "    cam = cv2.resize(cam, (224, 224))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = cam / np.max(cam)\n",
    "\n",
    "    #Return to BGR [0..255] from the preprocessed image\n",
    "    image = image[0, :]\n",
    "    image -= np.min(image)\n",
    "    image = np.minimum(image, 255)\n",
    "\n",
    "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "    cam = np.float32(cam) + np.float32(image)\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap\n",
    "\n",
    "#preprocessed_input = load_image(sys.argv[1])\n",
    "\n",
    "preprocessed_input = np.zeros((200, 224, 224, 3))\n",
    "\n",
    "k = 0\n",
    "for i in range(0, 3):\n",
    "  preprocessed_input[k] = dip.im_read(img[i]+\".jpg\")\n",
    "  k = k+1\n",
    "\n",
    "model = VGG16(weights='imagenet')\n",
    "#model = VGG16_MODEL\n",
    "\n",
    "#pre_inp = dip.im_read(img[10]+\".jpg\")\n",
    "\n",
    "predictions = model.predict(preprocessed_input)\n",
    "\n",
    "#predictions = model.predict(pre_inp)\n",
    "#top_1 = decode_predictions(predictions)[0][0]\n",
    "#print('Predicted class:')\n",
    "#print('%s (%s) with probability %.2f' % (top_1[1], top_1[0], top_1[2]))\n",
    "\n",
    "predicted_class = np.argmax(predictions)\n",
    "#predicted_class = history\n",
    "cam, heatmap = grad_cam(model, preprocessed_input, predicted_class, \"block5_conv3\")\n",
    "cv2.imwrite(\"gradcam.jpg\", cam)\n",
    "\n",
    "register_gradient()\n",
    "guided_model = modify_backprop(model, 'GuidedBackProp')\n",
    "saliency_fn = compile_saliency_function(guided_model)\n",
    "saliency = saliency_fn([preprocessed_input, 0])\n",
    "gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
    "cv2.imwrite(\"guided_gradcam.jpg\", deprocess_image(gradcam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "hPZwRhdwg8LS",
    "outputId": "f6bb2f8e-6ba4-4f82-f203-d3b5887793aa"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ca7c33789b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmissinglink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'missinglink'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.slim.nets import vgg\n",
    "\n",
    "import tensorflow as tf\n",
    "import missinglink\n",
    "\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "\n",
    "VGG16_MODEL=tf.keras.applications.VGG16(input_shape=img_shape,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "VGG16_MODEL.trainable=True\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(n_classes,activation='softmax')\n",
    "model = tf.keras.Sequential([\n",
    "  VGG16_MODEL,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])\n",
    "\n",
    "logits, end_points = vgg.vgg_16(inputs=input_placeholder)\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "session = tf.Session()\n",
    "# Download pre-trained checkpoint from http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\n",
    "tf.train.Saver().restore(session, 'vgg_16.ckpt')\n",
    "\n",
    "OWNER_ID = 'replace me with owner id'\n",
    "PROJECT_TOKEN = 'replace me with project token'\n",
    "\n",
    "missinglink_project = missinglink.TensorFlowProject(OWNER_ID, PROJECT_TOKEN)\n",
    "\n",
    "path = 'http://cmeimg-a.akamaihd.net/640/photos.demandstudios.com' + \\\n",
    "    '/getty/article/103/49/516464087.jpg'\n",
    "\n",
    "with missinglink_project.create_experiment() as experiment:\n",
    "    experiment.set_properties(\n",
    "        input_placeholder=input_placeholder.op.name,\n",
    "        output_layer=probs.op.name,\n",
    "        last_conv_layer=end_points['vgg_16/conv5/conv5_3'].op.name\n",
    "    )\n",
    "\n",
    "    experiment.generate_grad_cam(path, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDCAsOoCRwCb"
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "from pandas_ods_reader import read_ods\n",
    "import dippykit as dip\n",
    "\n",
    "path = \"./testing/labels_test.ods\"\n",
    "df = read_ods(path, 1, columns=[\"image\", \"label\"])\n",
    "img = df.iloc[:, 0].astype('str')\n",
    "k = 0\n",
    "for i in img:\n",
    "  if '.0' in i:\n",
    "    img[k] = i[0:len(i)-2]\n",
    "  #temp = dip.im_read(\"./testing/color_image/\"+img[k]+\".jpg\")\n",
    "  #temp = ndimage.median_filter(temp, size=15)\n",
    "  #dip.im_write(temp, \"testing/color_image/\"+img[k]+\".jpg\")\n",
    "  k = k+1\n",
    "\n",
    "path2 = \"./training/labels_train.ods\"\n",
    "df = read_ods(path2, 1, columns=[\"image\", \"label\"])\n",
    "img = df.iloc[:, 0].astype('str')\n",
    "k = 0\n",
    "for i in img:\n",
    "  if '.0' in i:\n",
    "    img[k] = i[0:len(i)-2]\n",
    "  #temp = dip.im_read(\"training/color_image/color_image/\"+img[k]+\".jpg\")\n",
    "  #temp = ndimage.median_filter(temp, size=15)\n",
    "  #dip.im_write(temp,\"training/\"+img[k]+\".jpg\")\n",
    "  k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjnWxWcIyRqE"
   },
   "outputs": [],
   "source": [
    "preprocessed_input = dip.im_read(\"./testing/color_image/\"+img[0]+\".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "VsKEd9K6BjgP",
    "outputId": "5ea9cba8-f4a6-4147-aa51-75b4acb24334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dippykit\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/2a/20a50295edc4b6de88244261b671eb3522e39020c177e42cbc5d0f5167a7/dippykit-2.0.3.tar.gz (52kB)\n",
      "\r",
      "\u001b[K     |██████▎                         | 10kB 18.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 30kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from dippykit) (3.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dippykit) (1.17.4)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from dippykit) (4.1.2.30)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from dippykit) (4.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from dippykit) (1.3.3)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from dippykit) (0.15.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->dippykit) (2.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->dippykit) (2.6.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->dippykit) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->dippykit) (1.1.0)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->dippykit) (0.46)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->dippykit) (2.4)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->dippykit) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image->dippykit) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->dippykit) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->dippykit) (42.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->dippykit) (4.4.1)\n",
      "Building wheels for collected packages: dippykit\n",
      "  Building wheel for dippykit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for dippykit: filename=dippykit-2.0.3-cp36-none-any.whl size=58952 sha256=cb3d9511df8dfcc76151a92f9a17c309f8838fdfc4f6584890cc0ab9e4f74c9f\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/42/9c/a692a4fe24e12db06f80e6e86dcc79cfe784ad832e9a5e96ff\n",
      "Successfully built dippykit\n",
      "Installing collected packages: dippykit\n",
      "Successfully installed dippykit-2.0.3\n",
      "Collecting pandas_ods_reader\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/f8/c1097f90411ec2cdfee9686c469a8acabff678185d761345975fc559c9fc/pandas_ods_reader-0.0.7-py3-none-any.whl\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas_ods_reader) (4.2.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pandas_ods_reader) (0.25.3)\n",
      "Collecting ezodf\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/c5/e966935c26d58d7e3d962270be61be972409084374d4093f478d1f82e8af/ezodf-0.3.2.tar.gz (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 4.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pandas_ods_reader) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->pandas_ods_reader) (2.6.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pandas_ods_reader) (1.17.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->pandas_ods_reader) (1.12.0)\n",
      "Building wheels for collected packages: ezodf\n",
      "  Building wheel for ezodf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ezodf: filename=ezodf-0.3.2-py2.py3-none-any.whl size=49003 sha256=cddaee2515f069e7ae0c786d325607c58081a945cce310a9b087185ff32be7d0\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/6c/f9/107d39d45441980bf273757eba003ef057c6193c9d7650fac7\n",
      "Successfully built ezodf\n",
      "Installing collected packages: ezodf, pandas-ods-reader\n",
      "Successfully installed ezodf-0.3.2 pandas-ods-reader-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install dippykit\n",
    "!pip install pandas_ods_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7sCsl-pcyFgD",
    "outputId": "39b724a4-c50e-4c5a-8957-ada3ff7389b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from pandas_ods_reader import read_ods\n",
    "import dippykit as dip \n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "path = \"training/labels_train.ods\"\n",
    "# Add code to read input and convert output values to labels\n",
    "df = read_ods(path, 1, columns=[\"image\", \"label\"])\n",
    "print(df.shape)\n",
    "img = df.iloc[:, 0].astype('str')\n",
    "label = df.iloc[:, 1]\n",
    "lab_train = np.zeros(200)\n",
    "k = 0\n",
    "im_train = []\n",
    "for i in img:\n",
    "  if '.0' in i:\n",
    "    img[k] = i[0:len(i)-2]\n",
    "  # im_train.append(\"BuildingDataset/training/color_image/\"+img[k]+\".jpg\")\n",
    "  im_train.append(\"training/color_image/\"+img[k]+\".jpg\")\n",
    "  lab_train[k] = label[k]*4\n",
    "  k = k+1\n",
    "\n",
    "path2 = \"testing/labels_test.ods\"\n",
    "df = read_ods(path2, 1, columns=[\"image\", \"label\"])\n",
    "img = df.iloc[:, 0].astype('str')\n",
    "label_t = df.iloc[:, 1]\n",
    "lab_test = np.zeros(50)\n",
    "k = 0\n",
    "im_test = []\n",
    "for i in img:\n",
    "  if '.0' in i:\n",
    "    img[k] = i[0:len(i)-2]\n",
    "  # im_test.append(\"BuildingDataset/testing/color_image/\"+img[k]+\".jpg\")\n",
    "  im_test.append(\"testing/color_image/\"+img[k]+\".jpg\")\n",
    "  lab_test[k] = label_t[k]*4\n",
    "  k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "azVaMouzgvLh",
    "outputId": "88fb3e2e-a656-4ad8-dc15-d53c95d1de3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5 steps, validate on 5 steps\n",
      "Epoch 1/25\n",
      "5/5 [==============================] - 303s 61s/step - loss: 4.2780 - acc: 0.3120 - val_loss: 1.7815 - val_acc: 0.1200\n",
      "Epoch 2/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.5143 - acc: 0.2720 - val_loss: 1.5870 - val_acc: 0.3000\n",
      "Epoch 3/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.5492 - acc: 0.3200 - val_loss: 1.6148 - val_acc: 0.2800\n",
      "Epoch 4/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4647 - acc: 0.3440 - val_loss: 1.7233 - val_acc: 0.2400\n",
      "Epoch 5/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4448 - acc: 0.3760 - val_loss: 1.6380 - val_acc: 0.2400\n",
      "Epoch 6/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4656 - acc: 0.3480 - val_loss: 1.6599 - val_acc: 0.2400\n",
      "Epoch 7/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4572 - acc: 0.3680 - val_loss: 1.5996 - val_acc: 0.2400\n",
      "Epoch 8/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4297 - acc: 0.3880 - val_loss: 1.6310 - val_acc: 0.2400\n",
      "Epoch 9/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4607 - acc: 0.3800 - val_loss: 1.5992 - val_acc: 0.2400\n",
      "Epoch 10/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4496 - acc: 0.3840 - val_loss: 1.6067 - val_acc: 0.2400\n",
      "Epoch 11/25\n",
      "5/5 [==============================] - 299s 60s/step - loss: 1.4432 - acc: 0.3880 - val_loss: 1.7319 - val_acc: 0.2200\n",
      "Epoch 12/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4223 - acc: 0.4000 - val_loss: 1.6015 - val_acc: 0.2200\n",
      "Epoch 13/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4404 - acc: 0.3680 - val_loss: 1.5686 - val_acc: 0.2200\n",
      "Epoch 14/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.3555 - acc: 0.4360 - val_loss: 1.5642 - val_acc: 0.2200\n",
      "Epoch 15/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.3528 - acc: 0.4160 - val_loss: 1.6343 - val_acc: 0.2400\n",
      "Epoch 16/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.5481 - acc: 0.3520 - val_loss: 1.5531 - val_acc: 0.2600\n",
      "Epoch 17/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.4775 - acc: 0.2360 - val_loss: 1.5309 - val_acc: 0.3200\n",
      "Epoch 18/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.3721 - acc: 0.3880 - val_loss: 1.5778 - val_acc: 0.2400\n",
      "Epoch 19/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.3658 - acc: 0.4240 - val_loss: 1.5558 - val_acc: 0.2600\n",
      "Epoch 20/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.3615 - acc: 0.3800 - val_loss: 1.5434 - val_acc: 0.2200\n",
      "Epoch 21/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.3845 - acc: 0.3840 - val_loss: 1.3958 - val_acc: 0.4200\n",
      "Epoch 22/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.3537 - acc: 0.3560 - val_loss: 1.3669 - val_acc: 0.2400\n",
      "Epoch 23/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.2853 - acc: 0.4560 - val_loss: 1.5392 - val_acc: 0.2400\n",
      "Epoch 24/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.2449 - acc: 0.4320 - val_loss: 1.6994 - val_acc: 0.2400\n",
      "Epoch 25/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.1874 - acc: 0.5000 - val_loss: 1.3618 - val_acc: 0.4200\n",
      "Train on 5 steps, validate on 5 steps\n",
      "Epoch 1/25\n",
      "5/5 [==============================] - 303s 61s/step - loss: 4.2780 - acc: 0.3120 - val_loss: 1.7815 - val_acc: 0.1200\n",
      "Epoch 2/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.5143 - acc: 0.2720 - val_loss: 1.5870 - val_acc: 0.3000\n",
      "Epoch 3/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.5492 - acc: 0.3200 - val_loss: 1.6148 - val_acc: 0.2800\n",
      "Epoch 4/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4647 - acc: 0.3440 - val_loss: 1.7233 - val_acc: 0.2400\n",
      "Epoch 5/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4448 - acc: 0.3760 - val_loss: 1.6380 - val_acc: 0.2400\n",
      "Epoch 6/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4656 - acc: 0.3480 - val_loss: 1.6599 - val_acc: 0.2400\n",
      "Epoch 7/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4572 - acc: 0.3680 - val_loss: 1.5996 - val_acc: 0.2400\n",
      "Epoch 8/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4297 - acc: 0.3880 - val_loss: 1.6310 - val_acc: 0.2400\n",
      "Epoch 9/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4607 - acc: 0.3800 - val_loss: 1.5992 - val_acc: 0.2400\n",
      "Epoch 10/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4496 - acc: 0.3840 - val_loss: 1.6067 - val_acc: 0.2400\n",
      "Epoch 11/25\n",
      "5/5 [==============================] - 299s 60s/step - loss: 1.4432 - acc: 0.3880 - val_loss: 1.7319 - val_acc: 0.2200\n",
      "Epoch 12/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4223 - acc: 0.4000 - val_loss: 1.6015 - val_acc: 0.2200\n",
      "Epoch 13/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.4404 - acc: 0.3680 - val_loss: 1.5686 - val_acc: 0.2200\n",
      "Epoch 14/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.3555 - acc: 0.4360 - val_loss: 1.5642 - val_acc: 0.2200\n",
      "Epoch 15/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.3528 - acc: 0.4160 - val_loss: 1.6343 - val_acc: 0.2400\n",
      "Epoch 16/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.5481 - acc: 0.3520 - val_loss: 1.5531 - val_acc: 0.2600\n",
      "Epoch 17/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.4775 - acc: 0.2360 - val_loss: 1.5309 - val_acc: 0.3200\n",
      "Epoch 18/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.3721 - acc: 0.3880 - val_loss: 1.5778 - val_acc: 0.2400\n",
      "Epoch 19/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.3658 - acc: 0.4240 - val_loss: 1.5558 - val_acc: 0.2600\n",
      "Epoch 20/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.3615 - acc: 0.3800 - val_loss: 1.5434 - val_acc: 0.2200\n",
      "Epoch 21/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.3845 - acc: 0.3840 - val_loss: 1.3958 - val_acc: 0.4200\n",
      "Epoch 22/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.3537 - acc: 0.3560 - val_loss: 1.3669 - val_acc: 0.2400\n",
      "Epoch 23/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.2853 - acc: 0.4560 - val_loss: 1.5392 - val_acc: 0.2400\n",
      "Epoch 24/25\n",
      "5/5 [==============================] - 300s 60s/step - loss: 1.2449 - acc: 0.4320 - val_loss: 1.6994 - val_acc: 0.2400\n",
      "Epoch 25/25\n",
      "5/5 [==============================] - 301s 60s/step - loss: 1.1874 - acc: 0.5000 - val_loss: 1.3618 - val_acc: 0.4200\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "img_shape = (224, 224, 3)\n",
    "n_classes = 5\n",
    "VGG16_MODEL=tf.keras.applications.VGG16(input_shape=img_shape,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "VGG16_MODEL.trainable=True\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(n_classes,activation='softmax')\n",
    "model = tf.keras.Sequential([\n",
    "  VGG16_MODEL,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])\n",
    "learning_rate = 0.0005\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[\"accuracy\"])\n",
    "# from skimage.util import random_noise\n",
    "BATCH_SIZE = 50\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "def _parse_data(x,y):\n",
    "  # image = dip.im_read(x)\n",
    "  # image = dip.adjustments.image_noise(image, 's&p')\n",
    "  # image = tf.convert_to_tensor(image)\n",
    "  image = tf.read_file(x)\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = (image/127.5) - 1\n",
    "  # image = tf.keras.layers.GaussianNoise(image)\n",
    "  # image = random_noise(image, 's&p')\n",
    "  return image, y\n",
    "  \n",
    "def _input_fn(x,y):\n",
    "  s = len(y)\n",
    "  ds=tf.data.Dataset.from_tensor_slices((x,y))\n",
    "  ds=ds.map(_parse_data)\n",
    "  ds=ds.shuffle(s)\n",
    "  ds = ds.repeat() \n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "  return ds\n",
    "  \n",
    "train_ds=_input_fn(im_train,lab_train)\n",
    "validation_ds=_input_fn(im_test,lab_test)\n",
    "history = model.fit(train_ds,\n",
    "                    epochs=25, \n",
    "                    steps_per_epoch=5,\n",
    "                    validation_steps=5,\n",
    "                    validation_data=validation_ds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "gradcam.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
